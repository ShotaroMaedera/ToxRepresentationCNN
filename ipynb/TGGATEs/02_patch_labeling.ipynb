{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "root = \"..\"\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path:\n",
    "  info_path: /data2/backup/maedera-workspace/TGGATE/rep/projects/2104projects/info/210403info.csv\n",
    "  ft_list_path:\n",
    "  fold_path: /data2/backup/maedera-workspace/TGGATE/rep/projects/2104projects/experiments/210403effnetb4-baseline/fold.csv\n",
    "  trained_path: \n",
    "  tile_dir: /data0/TGGATE/tiles\n",
    "  save_path: ../experiments/210807effnetb4-baseline-singleft\n",
    "log:\n",
    "  logger_name: 210807log.log\n",
    "random:\n",
    "  random_state: 123\n",
    "CV:  \n",
    "  create_fold: False\n",
    "  n_splits: 5\n",
    "  split_type: GroupKFold\n",
    "  metric_list:\n",
    "    - macro_auroc\n",
    "    - macro_accuracy\n",
    "  fold_list:\n",
    "    - 0\n",
    "model:\n",
    "  model_name: tf_efficientnet_b4_ns\n",
    "  feature_extraction: False\n",
    "  pretrained: True\n",
    "  load_trained: False\n",
    "preprocess:\n",
    "  image_size: 512\n",
    "  transform_list:\n",
    "    - HorizontalFlip\n",
    "    - VerticalFlip\n",
    "  n_tiles: 100\n",
    "process:\n",
    "  num_workers: 4\n",
    "  train_batch_size: 32\n",
    "  valid_batch_size: 32\n",
    "training:\n",
    "  criterion_name: BCE\n",
    "  optimizer_name: Adam\n",
    "  learning_rate: 0.0005\n",
    "  n_epochs: 20\n",
    "  verbose: 20\n",
    "device:\n",
    "  device: cuda\n",
    "debug: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmodel.train import train_loop\n",
    "from clmodel.utils import Logger\n",
    "from clmodel.dataset import load_features\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "info_path = \"../data/TGGATEs/processed/info.csv\"\n",
    "ft_list_path = \"../data/TGGATEs/processed/ft_list.txt\"\n",
    "\n",
    "info_df = pd.read_csv(info_path)\n",
    "ft_list = load_features(ft_list_path)\n",
    "\n",
    "info_df[\"GROUP\"] = 100 * info_df[\"EXP_ID\"] + info_df[\"GROUP_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from clmodel.utils import fix_seed\n",
    "\n",
    "torch.backends.cudnn.benchmark=True\n",
    "random_state = 123\n",
    "fix_seed(random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(name = logger_name, save_dir = save_path)\n",
    "\n",
    "fold = Fold(info_df, \"INDV_ID\", [\"FILE\"], ft_list, group_col = \"GROUP\",\n",
    "        random_state = random_state, split_type = split_type, n_splits = n_splits,\n",
    "        save_path = os.path.join(save_path, \"fold.csv\"))\n",
    "\n",
    "metrics = []\n",
    "for mtr_name in metric_list:\n",
    "    if mtr_name == \"macro_auroc\":\n",
    "        mtr = Metric(\"macro AUROC\", macro_auroc, \"+\", limit = (0, 1))\n",
    "        metrics.append(mtr)\n",
    "    elif mtr_name == \"auroc\":\n",
    "        mtr = Metric(\"AUROC\", auroc, \"multilabel\", limit = (0, 1))\n",
    "        metrics.append(mtr)\n",
    "    elif mtr_name == \"macro_accuracy\":\n",
    "        mtr = Metric(\"macro acc\", macro_accuracy, \"+\", limit = (0, 1))\n",
    "        metrics.append(mtr)\n",
    "    elif mtr_name == \"accuracy\":\n",
    "        mtr = Metric(\"acc\", accuracy, \"multilabel\", limit = (0, 1))\n",
    "        metrics.append(mtr)\n",
    "    elif mtr_name == \"macro_balanced_accuracy\":\n",
    "        mtr = Metric(\"macro balanced acc\", macro_balanced_accuracy, \"+\", limit = (0, 1))\n",
    "        metrics.append(mtr)\n",
    "    elif mtr_name == \"macro_r2_score\":\n",
    "        mtr = Metric(\"macro r2 score\", macro_r2_score, \"+\", limit = (0, 1))\n",
    "        metrics.append(mtr)\n",
    "\n",
    "CV = CrossValidation(fold, metrics = metrics)\n",
    "\n",
    "model = Model(model_name, len(ft_list),\n",
    "    feature_extraction=feature_extraction, pretrained=pretrained)\n",
    "if load_trained:\n",
    "    model.load_state_dict(torch.load(trained_path))\n",
    "print(\"created the model.\")\n",
    "train_transform = [albu.RandomCrop(image_size, image_size)]\n",
    "\n",
    "for transform in transform_list:\n",
    "    if transform == \"HorizontalFlip\":\n",
    "        train_transform.append(albu.HorizontalFlip(p=0.5))\n",
    "    elif transform == \"VerticalFlip\":\n",
    "        train_transform.append(albu.VerticalFlip(p=0.5))\n",
    "\n",
    "train_transform.append(albu.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                    ))\n",
    "\n",
    "train_transform.append(ToTensorV2())\n",
    "\n",
    "train_transform = albu.Compose(train_transform)\n",
    "\n",
    "valid_transform = albu.Compose([albu.RandomCrop(image_size, image_size),\n",
    "                    albu.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                    ),\n",
    "                    ToTensorV2(),])\n",
    "\n",
    "if tile_dict_flag:\n",
    "    from glob import glob\n",
    "    tile_dict = {}\n",
    "    for td in tile_dir:\n",
    "        svss = [f.split(\"/\")[-1] for f in glob(td+\"/*.svs\") if len(f.split(\".\"))==2]\n",
    "        for s in svss:\n",
    "            tile_dict[s] = td\n",
    "\n",
    "for fold_i in fold_list:\n",
    "    train_info, valid_info = CV[fold_i]\n",
    "    if tile_dict_flag:\n",
    "        train_dataset= TGGATEDataset(train_info, mode, tile_dir=None, n_tiles=n_tiles, tile_dict=tile_dict,\n",
    "            ft_list = ft_list, compound_list = None, pp_list = None, \n",
    "            transform = train_transform, debug = debug, he_aug=False)\n",
    "            \n",
    "        valid_dataset= TGGATEDataset(valid_info, mode, tile_dir=None, n_tiles=n_tiles, tile_dict=tile_dict,\n",
    "            ft_list = ft_list, compound_list = None, pp_list = None, \n",
    "            transform = valid_transform, debug = debug, he_aug=False)\n",
    "    else:\n",
    "        print(tile_dict_flag)\n",
    "        #with open(\"../processed_data/210417vein.pkl\", \"rb\") as f:\n",
    "        #    st = pickle.load(f)\n",
    "        train_dataset= TGGATEDataset(train_info, mode, tile_dir, n_tiles,\n",
    "            ft_list = ft_list, compound_list = None, pp_list = None, \n",
    "            transform = train_transform, debug = debug, he_aug=False, normalize=False)\n",
    "            \n",
    "        valid_dataset= TGGATEDataset(valid_info, mode, tile_dir, n_tiles,\n",
    "            ft_list = ft_list, compound_list = None, pp_list = None, \n",
    "            transform = valid_transform, debug = debug, normalize=False)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, num_workers=num_workers,\n",
    "        batch_size=train_batch_size,\n",
    "        drop_last=True,\n",
    "        #shuffle=True)\n",
    "        sampler=TGGATEBalancedSampler(train_dataset, 0.1))\n",
    "    valid_loader = DataLoader(dataset=valid_dataset, num_workers=num_workers,\n",
    "        batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "    if criterion_name == \"BCE\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        preprocess = lambda x: x.sigmoid()\n",
    "    elif criterion_name == \"WeightedBCE\":\n",
    "        positive_weight = train_info[ft_list].mean().values.astype(np.float32)\n",
    "        positive_weight = torch.tensor((1 - positive_weight)/(positive_weight + 1e-5))\n",
    "        criterion = WeightedBCELossWithLogits(positive_weight=positive_weight, negative_weight=torch.tensor(1), device=device)\n",
    "        preprocess = lambda x: x.sigmoid()\n",
    "    elif criterion_name == \"FocalBCE\":\n",
    "        criterion = FocalBCELossWithLogits(gamma=1)\n",
    "        preprocess = lambda x: x.sigmoid()\n",
    "    elif criterion_name == \"MSE\":\n",
    "        criterion = nn.MSELoss()\n",
    "        preprocess = lambda x: x\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, n_epochs, 5e-7)\n",
    "    model = model.to(device)\n",
    "    train_history, valid_history, best_score, best_predictions = \\\n",
    "        train_loop(model, train_loader, valid_loader, fold_i, criterion,\n",
    "        optimizer, device, n_epochs, scheduler, metrics, save_path,\n",
    "        model_name, preprocess, verbose, logger)\n",
    "    print(best_score)\n",
    "    visualize(train_history, valid_history, metrics, save_dir=save_path)\n",
    "    for mtr_name, predictions in best_predictions.items():\n",
    "        CV.set_oof_pred(fold_i, predictions[0], mtr_name)\n",
    "    CV.save_oof(os.path.join(save_path, \"oof_df.csv\"))\n",
    "\n",
    "    with open(os.path.join(save_path, f\"train_result_fold{fold_i}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump((train_history, valid_history, best_score, best_predictions), f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
